# Predicci√≥n de Fuga de Clientes (Telecom Churn Prediction) üì°

Este repositorio contiene la soluci√≥n a la **Actividad 1** de la asignatura **Machine Learning II**. El proyecto consiste en la construcci√≥n y evaluaci√≥n de modelos de clasificaci√≥n supervisada para predecir la probabilidad de que un cliente abandone una empresa de telecomunicaciones ("Churn").

## üìÑ Contexto del Problema

Una empresa de telecomunicaciones busca identificar tempranamente a los clientes con alto riesgo de fuga para dise√±ar campa√±as de retenci√≥n efectivas. Se trabaj√≥ con un dataset hist√≥rico que incluye caracter√≠sticas demogr√°ficas, de servicios contratados y de facturaci√≥n.

* **Variable Objetivo:** `Churn` (1 = Se va, 0 = Se queda).
* **Desaf√≠o Principal:** El desbalance de clases (la tasa de fuga es aprox. 26.5%) y la necesidad de capturar relaciones no lineales entre variables.

## üõ†Ô∏è Tecnolog√≠as Utilizadas

El proyecto fue desarrollado en **Python** utilizando el siguiente stack tecnol√≥gico:

* **Pandas & NumPy:** Limpieza, manipulaci√≥n de datos e ingenier√≠a de caracter√≠sticas.
* **Scikit-learn:**
    * *Preprocesamiento:* `StandardScaler`, `OneHotEncoder`, `SimpleImputer`.
    * *Modelado:* `LogisticRegression`.
    * *Ingenier√≠a de Features:* `PolynomialFeatures`.
    * *Selecci√≥n de Modelos:* `GridSearchCV`, `StratifiedKFold`.
    * *M√©tricas:* AUC-ROC, Precision-Recall, F1-Score, Matriz de Confusi√≥n.
* **Matplotlib & Seaborn:** Visualizaci√≥n de datos y evaluaci√≥n de modelos.

## üöÄ Metodolog√≠a

El flujo de trabajo implementado se divide en 4 etapas principales:

1.  **Preprocesamiento y Limpieza:**
    * Tratamiento de valores nulos en variables num√©ricas (`TotalCharges`).
    * Codificaci√≥n de variables categ√≥ricas mediante `OneHotEncoder`.
    * Estandarizaci√≥n de variables num√©ricas.

2.  **Modelado Iterativo:**
    * **Modelo Base:** Regresi√≥n Log√≠stica est√°ndar sin penalizaci√≥n.
    * **Modelo Polinomial:** Generaci√≥n de interacciones de grado 2 para capturar relaciones complejas entre variables num√©ricas.
    * **Modelo Regularizado:** Aplicaci√≥n de penalizaciones **L1 (Lasso)** y **L2 (Ridge)** para controlar la complejidad y evitar el sobreajuste.

3.  **Optimizaci√≥n:**
    * Uso de `GridSearchCV` para encontrar el hiperpar√°metro √≥ptimo de regularizaci√≥n ($C$).

## üìä Resultados Clave

El mejor modelo seleccionado fue la **Regresi√≥n Log√≠stica con Transformaciones Polinomiales y Regularizaci√≥n L1 (Lasso)**.

| M√©trica | Resultado | Interpretaci√≥n |
| :--- | :--- | :--- |
| **AUC-ROC** | **0.8475** | Alta capacidad del modelo para distinguir entre clientes que se van y los que se quedan. |
| **Accuracy** | 0.81 | Un buen rendimiento general, aunque influenciado por el desbalance de clases. |
| **Recall (Churn)**| 0.54 | Se logra detectar al 54% del total de fugas reales. |
| **Precision** | 0.67 | De los clientes alertados como riesgo, el 67% efectivamente abandon√≥ la empresa. |

### üí° An√°lisis de Regularizaci√≥n
La selecci√≥n autom√°tica de la penalizaci√≥n **L1 (Lasso)** fue cr√≠tica para el √©xito del modelo. De las 36 variables generadas (por los polinomios), **el modelo elimin√≥ autom√°ticamente 6 variables** (asign√°ndoles coeficiente 0), reduciendo la complejidad en un **16.7%** y filtrando el ruido.

## üìÇ Estructura del Repositorio

* `actividad1_ML2.ipynb`: Jupyter Notebook con el c√≥digo completo, desde la carga de datos hasta la evaluaci√≥n final.
* `data-churn.csv`: Dataset utilizado.
* `README.md`: Documentaci√≥n del proyecto.

## üíª Instrucciones de Ejecuci√≥n

1. Clonar este repositorio.
2. Instalar las dependencias necesarias:
   ```bash
   pip install pandas numpy scikit-learn matplotlib seaborn
